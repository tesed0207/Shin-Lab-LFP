{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b3433fe-24ec-42f9-bccc-1ade13584983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load LFP data\n",
    "import numpy as np\n",
    "import gc\n",
    "stim_num = [110000, 110101, 110105, 110106, 110107, 110109, 110110, 110111, 110506, 110511, 111105, 111109, 111201, 111299, 111301, 111302, 111303, 111304, 111305, 111306, 111307, 111308]\n",
    "sub_sessions = [(\"sub-619296\", \"ses-1187930705\"), (\"sub-620333\", \"ses-1188137866\"), (\"sub-620334\", \"ses-1189887297\"), (\"sub-625545\", \"ses-1182865981\"), (\"sub-625554\", \"ses-1181330601\"), (\"sub-625555\", \"ses-1183070926\"), (\"sub-630506\", \"ses-1192952695\"), (\"sub-631510\", \"ses-1196157974\"), (\"sub-631570\", \"ses-1194857009\"), (\"sub-633229\", \"ses-1199247593\"), (\"sub-637484\", \"ses-1208667752\")]\n",
    "session_num = 3\n",
    "stim_windows = [] # (trial, time, channel)\n",
    "probe_num = 2\n",
    "stim_windows2 = []\n",
    "probe_num2 = 3\n",
    "for frame_num in range(len(stim_num)) : \n",
    "    stim_windows.append(np.load(f'../material/LFP_npy_data/{sub_sessions[session_num][0]}_{sub_sessions[session_num][1]}/probe{probe_num}_frame{frame_num}.npy'))\n",
    "    stim_windows2.append(np.load(f'../material/LFP_npy_data/{sub_sessions[session_num][0]}_{sub_sessions[session_num][1]}/probe{probe_num2}_frame{frame_num}.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61160a4a-606b-4def-8359-4ee364dd7f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4960"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data\n",
    "import random\n",
    "frame_stimtype = [(3, 'IC1'), (7, 'IC2'), (8, 'IRE1'), (9, 'IRE2')]\n",
    "\n",
    "data = [] # (trial #, channel, time)\n",
    "labels = []\n",
    "\n",
    "for i, (frame_num, stimtype) in enumerate(frame_stimtype) :\n",
    "    trial_nums = np.arange(stim_windows[frame_num].shape[0])\n",
    "    random.shuffle(trial_nums)\n",
    "    for j, trial_num in enumerate(trial_nums) :\n",
    "        if j == 50 : break\n",
    "        temp_data = np.concatenate((stim_windows[frame_num][trial_num,:,:], stim_windows2[frame_num][trial_num,:,:]), axis = 1)\n",
    "        data.append(temp_data)\n",
    "        labels.append(i)\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "del stim_windows\n",
    "del stim_windows2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db8a88-cf9a-40ed-a911-55159eedf32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch: 49, G_Loss: 0.000375, D_Loss: 4.303887, time: 0.68 min\n",
      "Epoch: 99, G_Loss: 0.000363, D_Loss: 4.319828, time: 1.37 min\n",
      "Epoch: 149, G_Loss: 0.000354, D_Loss: 4.332950, time: 2.06 min\n",
      "Epoch: 199, G_Loss: 0.000261, D_Loss: 4.484851, time: 2.74 min\n",
      "Epoch: 249, G_Loss: 0.000257, D_Loss: 4.492510, time: 3.43 min\n",
      "Epoch: 299, G_Loss: 0.000255, D_Loss: 4.495725, time: 4.13 min\n",
      "Epoch: 349, G_Loss: 0.000254, D_Loss: 4.497519, time: 4.84 min\n",
      "Epoch: 399, G_Loss: 0.000253, D_Loss: 4.499499, time: 5.52 min\n",
      "Epoch: 449, G_Loss: 0.000253, D_Loss: 4.501042, time: 6.20 min\n",
      "Epoch: 499, G_Loss: 0.000252, D_Loss: 4.502145, time: 6.89 min\n",
      "Epoch: 549, G_Loss: 0.000252, D_Loss: 4.502733, time: 7.57 min\n",
      "Epoch: 599, G_Loss: 0.000251, D_Loss: 4.503318, time: 8.26 min\n",
      "Epoch: 649, G_Loss: 0.000251, D_Loss: 4.504123, time: 8.94 min\n",
      "Epoch: 699, G_Loss: 0.000251, D_Loss: 4.504537, time: 9.63 min\n",
      "Epoch: 749, G_Loss: 0.000251, D_Loss: 4.504989, time: 10.32 min\n",
      "Epoch: 799, G_Loss: 0.000250, D_Loss: 4.505628, time: 11.00 min\n",
      "Epoch: 849, G_Loss: 0.000250, D_Loss: 4.506471, time: 11.68 min\n",
      "Epoch: 899, G_Loss: 0.000249, D_Loss: 4.507278, time: 12.37 min\n",
      "Epoch: 949, G_Loss: 0.000249, D_Loss: 4.508232, time: 13.05 min\n",
      "Epoch: 999, G_Loss: 0.000249, D_Loss: 4.509088, time: 13.73 min\n",
      "Epoch: 1049, G_Loss: 0.000248, D_Loss: 4.509996, time: 14.42 min\n",
      "Epoch: 1099, G_Loss: 0.000248, D_Loss: 4.511119, time: 15.10 min\n",
      "Epoch: 1149, G_Loss: 0.000247, D_Loss: 4.512092, time: 15.80 min\n",
      "Epoch: 1199, G_Loss: 0.000247, D_Loss: 4.512920, time: 16.49 min\n",
      "Epoch: 1249, G_Loss: 0.000246, D_Loss: 4.513752, time: 17.17 min\n",
      "Epoch: 1299, G_Loss: 0.000246, D_Loss: 4.514505, time: 19.07 min\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sn \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(device)\n",
    "\n",
    "class CSDDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32).unsqueeze(1).to(device)  # (200, 400, 84) -> (200, 1, 400, 84)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, channel_num, time_len, class_num, z_size = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.z_size = z_size\n",
    "        self.channel_num = channel_num\n",
    "        self.time_len = time_len\n",
    "        self.class_num = class_num\n",
    "\n",
    "        self.label_emb = nn.Embedding(self.class_num, self.class_num)\n",
    "\n",
    "        self.linear1 = nn.Linear(z_size+class_num, 256)\n",
    "        self.linear2 = nn.Linear(256, 512)\n",
    "        self.linear3 = nn.Linear(512, 1024)\n",
    "        self.linear4 = nn.Linear(1024, channel_num * time_len)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        x = torch.cat((self.label_emb(labels), z), -1)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.relu(self.linear3(x))\n",
    "        x = self.linear4(x)\n",
    "        x = self.tanh(x) #sigmoid보다 더 또렷한 이미지를 만든다고 함\n",
    "        x = x.view(x.size(0), 1, self.channel_num, self.time_len)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module) :\n",
    "    def __init__(self, channel_num, time_len, class_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channel_num = channel_num\n",
    "        self.time_len = time_len\n",
    "        self.class_num = class_num\n",
    "\n",
    "        self.label_emb = nn.Embedding(self.class_num, self.class_num)\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.channel_num * self.time_len + self.class_num, 1024)\n",
    "        self.linear2 = nn.Linear(1024, 512)\n",
    "        self.linear3 = nn.Linear(512, 256)\n",
    "        self.linear4 = nn.Linear(256, 1)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2) #보통 generator에는 relu, discrimiator에는 leaky relu(gradient의 소실 방지)를 사용하는데 이유는 잘 모르겠음\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        x = torch.cat((x.view(x.size(0),-1),self.label_emb(labels)),-1)\n",
    "        x = self.leaky_relu(self.linear1(x))\n",
    "        x = self.leaky_relu(self.linear2(x))\n",
    "        x = self.leaky_relu(self.linear3(x))\n",
    "        x = self.linear4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "dataset = CSDDataset(data, labels)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size = 10, shuffle = True)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 2000\n",
    "\n",
    "generator = Generator(data.shape[2], data.shape[1], len(frame_stimtype)).to(device)\n",
    "discriminator = Discriminator(data.shape[2], data.shape[1], len(frame_stimtype)).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_history={'gen':[],\n",
    "              'dis':[]}\n",
    "\n",
    "# Train\n",
    "batch_count = 0\n",
    "start_time = time.time()\n",
    "discrimiator.train()\n",
    "generator.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for xb, yb in train_loader:\n",
    "        batch_size = xb.shape[0]\n",
    "        \n",
    "        yb_real = torch.Tensor(batch_size, 1).fill_(1.0).to(device) # real_label\n",
    "        yb_fake = torch.Tensor(batch_size, 1).fill_(0.0).to(device) # fake_label\n",
    "        \n",
    "        # Genetator\n",
    "        generator.zero_grad()\n",
    "        z = torch.randn(batch_size,100).to(device) # 노이즈 생성\n",
    "        gen_label = torch.randint(0,4,(batch_size,)).to(device) # label 생성\n",
    "\n",
    "         # 가짜 이미지 생성\n",
    "        out_gen = generator(z, gen_label)\n",
    "\n",
    "        # 가짜 이미지 판별\n",
    "        out_dis = discrimiator(out_gen, gen_label)\n",
    "\n",
    "        loss_gen = criterion(out_dis, yb_real)\n",
    "        loss_gen.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # Discriminator\n",
    "        discrimiator.zero_grad()\n",
    "        \n",
    "        # 진짜 이미지 판별\n",
    "        out_dis = discrimiator(xb, yb)\n",
    "        loss_real = criterion(out_dis, yb_real)\n",
    "\n",
    "        # 가짜 이미지 판별\n",
    "        out_dis = discrimiator(out_gen.detach(),gen_label)\n",
    "        loss_fake = criterion(out_dis,yb_fake)\n",
    "\n",
    "        loss_dis = (loss_real + loss_fake) / 2\n",
    "        loss_dis.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        loss_history['gen'].append(loss_gen.item())\n",
    "        loss_history['dis'].append(loss_dis.item())\n",
    "\n",
    "        batch_count += 1\n",
    "        if batch_count % 1000 == 0:\n",
    "            print('Epoch: %.0f, G_Loss: %.6f, D_Loss: %.6f, time: %.2f min' %(epoch, loss_gen.item(), loss_dis.item(), (time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cd71f9-e7f4-4213-9d5d-020851f23519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4bdf27-51da-426b-b5a7-86c6f82af166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss history\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('Loss Progress')\n",
    "plt.plot(loss_history['gen'], label='Gen. Loss')\n",
    "plt.plot(loss_history['dis'], label='Dis. Loss')\n",
    "plt.xlabel('batch count')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b56a3-6e6e-4526-962a-c427e64e132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#가중치 저장\n",
    "path2models = './models/'\n",
    "os.makedirs(path2models, exist_ok=True)\n",
    "path2weights_gen = os.path.join(path2models, 'weights_gen.pt')\n",
    "path2weights_dis = os.path.join(path2models, 'weights_dis.pt')\n",
    "\n",
    "torch.save(generator.state_dict(), path2weights_gen)\n",
    "torch.save(discriminator.state_dict(), path2weights_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faa7686-0530-4ea5-a27d-0de78c1711bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 불러오기\n",
    "weights = torch.load(path2weights_gen)\n",
    "generator.load_state_dict(weights)\n",
    "\n",
    "# evalutaion mode\n",
    "generator.eval()\n",
    "\n",
    "# fake image 생성\n",
    "with torch.no_grad():\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    cols, rows = 4, 4 # row와 col 갯수\n",
    "    for i in range(rows * cols):\n",
    "        fixed_noise = torch.randn(16, 100, device=device)\n",
    "        label = torch.randint(0,4,(16,), device=device)\n",
    "        img_fake = generator(fixed_noise, label).detach().cpu()\n",
    "        fig.add_subplot(rows, cols, i+1)\n",
    "        plt.title(label[i].item())\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img_fake[i].squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17477adc-40bd-4479-90cc-8dca0d13f5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf776b-19ae-4645-b8a1-ed588238aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# GPU 메모리 초기 상태\n",
    "initial_memory = torch.cuda.memory_allocated()\n",
    "print(f\"Initial GPU memory usage: {initial_memory / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "# 모델 생성 후 GPU 메모리 확인\n",
    "model = CNN(3, 3).to(device)\n",
    "post_model_memory = torch.cuda.memory_allocated()\n",
    "print(f\"After model allocation: {post_model_memory / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "# 입력 데이터 예시\n",
    "example_input = torch.randn(1, 1, 400, 168).to(device)\n",
    "\n",
    "# 모델을 사용한 후 GPU 메모리 확인\n",
    "_ = model(example_input)\n",
    "post_inference_memory = torch.cuda.memory_allocated()\n",
    "print(f\"After inference: {post_inference_memory / (1024 ** 2):.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
